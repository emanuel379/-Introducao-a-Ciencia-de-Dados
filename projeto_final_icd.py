# -*- coding: utf-8 -*-
"""Projeto Final ICD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dortq_RHHMF40OBNT-4ED2p2_t2MlCGi

## SME0828 - Introdução à Ciência de Dados

### Projeto Final - Previsão de preços de casas a partir de suas características

##### Integrantes:
Ada Maris Pereira Mário ---- 12725432

Emanuel Victor da Silva Favorato ---- 12558151

Katiely Feitosa de Lacerda ---- 12777100

### Introdução

O projeto em questão tem como objetivo responder à pergunta: "É possível prever o preço de casas a partir de suas características?". Para isso, selecionamos uma base de dados do Departamento de Censo dos EUA com dados sobre o censo na Califórnia com 10 diferentes métricas para cada bloco desse estado.

O dataset está disponível neste [link](https://www.kaggle.com/datasets/shibumohapatra/house-price).

O dataset conta com 10 atributos, sendo eles:

**longitude:** Atributo numérico. Longitude da região;

**latitude:** Atributo numérico. Latitude da região;

**housing_median:** Atributo numérico. Mediana das idades das casas na região;

**total_rooms:** Atributo numérico. Total de cômodos, juntando todas as casas na região;

**total_bedrooms:** Atributo numérico. Total de quartos, juntando todas as casas na região;

**population:** Atributo numérico. População na região.

**households:** Atributo numérico. Casas para moradia na região;

**median_income:** Atributo numérico. Renda média das pessoas na região.

**ocean_proximity:** Atributo categórico. Proximidade com o oceano.

**median_house_value:** Atributo numérico. Mediana dos valores das casas.

### Bibliotecas
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from scipy.stats import variation
import statsmodels.api as sm
import plotly.express as px
from sklearn.decomposition import PCA
import scipy.stats as stats
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics

"""### Abertura e Preparação dos dados"""

df = pd.read_csv('housing.csv', header=(0))
df.head()

df.shape

"""Como visto, temos um conjunto de dados com 20640 instâncias e 10 atributos. Vejamos agora algumas informações e estatísticas básicas dos nossos dados:"""

df.info()

"""Vemos que o atributo "ocean_proximity" é o único nominal em meio aos demais numéricos."""

#informações do dataFrame
df.describe().round(2)

"""Aqui podemos notar algumas variáveis com variância alta, como "total_rooms", "total_bedrooms", "population" e a própria variável resposta "median_house_value". Entretanto, temos que ter em mente que, por se tratar de atributos com um range amplo, é natural que tais desvios também sejam amplos.


Por isso, vejamos o coeficiente de variação das covariáveis para verificar a dispersão dos valores de acordo com suas médias:
"""

variation(df.drop(columns='ocean_proximity'))

"""### Limpeza e organização dos dados

Inicialmente, vamos fazer uma preparação dos dados. Temos quase 21k de instâncias na planílha e algumas inconsistências.  Como teremos que fazer one-hot-encoding da variável categórica, vamos inicialmente padronizar suas respostas para terem todas letras minúsculas e assim as apresentações das colunas serem similares.
"""

# Vamos começar preparando as colunas com valores string

string_columns= list(df.dtypes[df.dtypes == 'object'].index)
string_columns

# Vamos deixar todos os atributos em minúsculo e substituir espaços por '_'

for col in string_columns:
    df[col] = df[col].str.lower().str.replace(' ', '_')

# Conferindo os dados novamente:

df.head()

"""

Agora, verificamos os valores nulos contidos no conjunto:"""

#número de valores ausentes em cada coluna
df.isna().sum()

"""Vamos substituir os valores NaN presentes no atributo "total_bedrooms" pela mediana desta coluna."""

#preenche os valores ausentes com a mediana
df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].median())

"""Conferindo novamente se ainda restam NaN:

Vamos verificar de novo os coeficientes de variação:
"""

variation(df.drop(columns='ocean_proximity'))

"""Finalmente, vamos converter os atributos categóricos em numéricos utilizando one-hot-encoding."""

#conferindo se está zerado
df.isna().sum()

df_original = df
df = pd.get_dummies(df)
df.head()

# Observando estatísticas importantes após as mudanças
df.describe().round(2)

"""### Análise exploratória dos dados

Inicialmente, vamos avaliar a correlação entre os atributos, a fim de analisar quais são relevantes para a determinação do preço da casa.
"""

plt.figure(figsize =(20, 10))
sns.heatmap(df.corr(), annot = True) # Mapa de calor, onde  annot insere os valores dentro das ilhas
plt.title('Correlação de Pearson')

"""Pela tabela acima podemos tirar algumas informações:
- A média dos valores das casas está bem ligado a sua proximidade com o oceano, (casas mais próximas ao oceano podem ser mais caras);

- O total de quartos está muito ligado ao total de banheiros, e essa correlação faz muito sentido;

- Tanto o total de quartos como o de banhieros estão muitos ligados a quantidade da população nessa área.

"""

plt.figure(figsize =(20, 10))
sns.heatmap(df.corr('spearman'), annot = True) # Mapa de calor, onde  annot insere os valores dentro das ilhas
plt.title('Correlação de Spearman')

"""Quando fazermos a Correlação de Spearman, podemos ver uma relação entre o total de quartos e a mediana das rendas das famílias.

Vemos que "median_income" é o atributo mais promissor para prever "median_house_value" em ambos os tipos de correlação.
Vejamos mais detalhadamente sua distribuição em função deste atributo.
"""

plt.figure(figsize=(10,6))
sns.scatterplot(x="median_income", y="median_house_value",data=df)

"""A linha reta na renda pode significar um limite superior para está variável, ou seja, um teto de valores.

Vejamos também a distribuição da renda conforme a localização das casas:
"""

# verificando a renda de acordo com a latitude e longitude

plt.figure(figsize=(15, 8))
sns.scatterplot(x="latitude", y="longitude",data=df, hue="median_income", palette="coolwarm")

"""Podemos ver que quando maior a renda, mais ao norte as pessoas moram.

Agora, a distribuição dos preços em função da localização das casas:
"""

# comparando a latitude e longitude com a mediana dos valores das casas
plt.figure(figsize=(15, 8))
sns.scatterplot(x="latitude", y="longitude", data=df, hue="median_house_value", palette="coolwarm")

"""Podemos observar que os maiores índices de renda mediana se concentram em áreas próximas à praia e os menores no interior, bem como a concentração populacional e as casas mais valiosas."""

# Observando a contagem dos dados
columns = df.columns.to_list()

for i in columns:
    sns.histplot(df[i])
    plt.show()

# Observando as funções de densidade
for i in columns:
    sns.distplot(df[i])
    plt.show()

plt.figure(figsize=(10, 9))

sns.histplot(df['median_house_value'], bins=100)
plt.ylabel('Frequência')
plt.xlabel('Preço')
plt.title('Distribuição dos preços (median_house_value)')

"""Dados com assimetria à direita, mas com uma variação muito alta no final. Este pico após os 500000 pode atrapalhar nossas predições.

### Identificando e removendo outliers
"""

for i in df.iloc[:,0:9]:
    fig, ax = plt.subplots()
    fig.set_size_inches(6,3)
    sns.boxplot(x=i,data=df,ax=ax)

"""Vemos que o total de quartos, o de banheiros a população na área, a médiana da renda, os households, e a mediana dos valores das casa estão com ouliers. Vamos retirar esses outliers que podem prejudicar as nossas previsões.


"""

s1 = df.shape
clean = df[['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']]
for i in clean.columns:
    qt1 = df[i].quantile(0.25)
    qt3 = df[i].quantile(0.75)
    iqr = qt3 - qt1
    lower = qt1 - (1.5 * iqr)
    upper = qt3 + (1.5 * iqr)
    min_in = df[df[i] < lower].index
    max_in = df[df[i] > upper].index
    df.drop(min_in, inplace=True)
    df.drop(max_in, inplace=True)
s2 = df.shape
outliers = s1[0] - s2[0]
print("Deleted outliers: ", outliers)

"""### Normalização dos dados

Primeiramente, vamos reordenar as colunas do dataframe:
"""

df = df[['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income','ocean_proximity_<1h_ocean',
       'ocean_proximity_inland', 'ocean_proximity_island',
       'ocean_proximity_near_bay', 'ocean_proximity_near_ocean','median_house_value']]
df.head()

# Convertendo para o formato Numpy

np_data = df.to_numpy()
nrow, ncol = df.shape

X = np_data[:,0:ncol-1]
y = np_data[:,-1]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(X)
X = scaler.transform(X)

print('Dados transformados:')
print('Media: ', np.mean(X, axis = 0))
print('Desvio Padrao:', np.std(X, axis = 0))

"""### Seleção de ajuste do modelo

Vamos, primeiro, separar o conjunto de dados em treino e teste:
"""

# Splitting the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Vamos começar fazendo uma regressão:"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Creating and fitting the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions
y_pred = model.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
r_squared = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r_squared}")

# Making predictions
y_pred = model.predict(X_test)

plt.figure(figsize=(10, 8))

sns.histplot(y_test, label='Target', color = "green")
sns.histplot(y_pred, label='Prediction', color='red')

plt.ylabel('Frequência')
plt.title('Distribuição dos preços - Regressão')

plt.legend()
plt.show()

"""vemos uma discrepância grande

Vamos utilizar a biblioteca Statsmodels para podermos visualizar com mais detalhes alguns parâmetros da nossa regressão:
"""

# Verificando a homoscedacidade
X = df.drop('median_house_value', axis=1)
y = df['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = sm.OLS(y_train, sm.add_constant(X_train)).fit()
residuals = model.resid

plt.scatter(model.predict(), residuals, color='red')
plt.axhline(y=0, color='black', linestyle='--')
plt.title('Gráfico da Homocedasticidade dos resíduos')
plt.xlabel('Valores ajustados')
plt.ylabel('Residuos')
plt.show()

# Teste de normalidade dos resíduos
sm.qqplot(residuals, line='s')
plt.title('QQ Plot para a normalidade dos resíduos')
plt.show()

# Definindo o modelo e observando estatísticas de interesse
modelo = sm.OLS(y_train, sm.add_constant(X_train)).fit()
print(modelo.summary())

# Teste de Kolmogorov para rejeitar, ou não, a normalidade dos resíduos
y = modelo.resid
media = np.mean(y)
std = np.std(y,ddof=1)

print('Sob H0 normal, p-valor=',stats.kstest(y,cdf='norm' , args=(media,std), N=len(y))[1])

"""Assim, rejeitamos a normalidade dos dados"""

# Visualize the regression line
plt.scatter(y_test, y_pred, color='black')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='red', linewidth=2)
plt.title('Actual vs Predicted values')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

"""Vamos, agora, fazer uma modelagem usando random forest:"""

from sklearn.ensemble import RandomForestRegressor

# Creating and training the Random Forest model
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)  # Set the desired number of trees (n_estimators)
random_forest_model.fit(X_train, y_train)

# Making predictions using the Random Forest model
y_pred_rf = random_forest_model.predict(X_test)

#Evaluating the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
r_squared_rf = r2_score(y_test, y_pred_rf)

print(f"Mean Squared Error (Random Forest): {mse_rf}")
print(f"R-squared (Random Forest): {r_squared_rf}")

# Plotting the actual values vs. Predicted values using the Random Forest model
plt.figure(figsize=(10, 8))

sns.histplot(y_test, label='Target', color = "green")
sns.histplot(y_pred_rf, label='Prediction', color='red')

plt.ylabel('Frequência')
plt.title('Distribuição dos preços - Random Forest')

plt.legend()
plt.show()

"""Podemos perceber que o Random Forest obteve um desempenho um pouco melhor que a Regressão, com um $MSE$ menor, um $R^2$ maior e, consequentemente, uma distribuição mais certeira comparada com os labels originais.

### Análise de componentes principais (PCA)
"""

# Análise de Componentes Principais
# Decompondo a matriz de variâncias e covariâncias em componentes principais.

X = np.matrix(df.iloc[:,0:10])
S = np.cov(np.transpose(X))
X = np.asarray(X)

# Coletando as variâncias

np.diagonal(S)

# Realizando um teste PCA com 8 componentes principais

pca = PCA(n_components=8)
pca

pca.fit(X)

# Pesos das componentes principais

pca.components_[0,:]

# Quantas componentes explicam ao menos 90% da variabilidade dos dados?

# Variância das componentes principais

pca.explained_variance_ratio_

np.round(pca.explained_variance_ratio_,2)

"""Apenas uma componente explicaria mais de 90% da variância dos dados"""

# Observândo a variância explicada acumulada

pca.explained_variance_ratio_.cumsum()

fig = plt.figure(figsize=(8,5))

plt.plot(pca.explained_variance_ratio_,  'ro-', linewidth=2)
plt.title('Gráfico de Cotovelo')
plt.xlabel('Componente Principal')
plt.ylabel('Autovalor')

plt.show()

"""Pelo gráfico de cotovelo observamos que duas componentes já seriam suficientes para explicar a variância dos dados."""

pca.transform(X)

# Considerando as 2 componentes principais

pca = PCA(n_components=2)
pca.fit(X)

pca.components_

"""### Considerações Finais

Neste trabalho de Ciência de Dados, utilizamos a base de dados do censo na Califórnia para ajustar um modelo que preveja o valor de casas de acordo com sua respectiva localização, número de cômodos, população da região, entre outros atributos. Para isso, utilizamos dois métodos distintos: Regressão Linear e Random Forest.

O modelo de regressão linear apresentou um Mean Squared Error $MSE = 3277349121,73$ e um coeficiente de determinação $R^2 = 0,6097$. Enquanto que o Random Forest obteve um $MSE = 1906218891,29$ e $R^2 = 0,7729$. É importante frisar que estamos trabalhando com grandezas de natureza ampla que são os preços de casas, o que explica em parte o alto valor dos erros.

Assim, vemos que a abordagem do Random Forest proporcionou uma melhoria significativa nos resultados, demonstrando ser mais eficaz em capturar a complexidade dos dados, resultando em uma melhor capacidade de prever os preços das casas. A capacidade do modelo de lidar com interações complexas entre os atributos foi crucial para melhorar a precisão das previsões. Por outro lado, recomenda-se a realização de uma validação cruzada mais robusta e a otimização dos hiperparâmetros do Random Forest para levar a melhorias adicionais.

Outrossim, uma consideração importante para a análise do desempenho do método de Regressão Linear é a verificação de uma relação não-linear entre os dados, da multicolinearidade entre as covariáveis (como visto, por exemplo, na relação renda x localização), a não-normalidade dos erros (verificado no Gráfico de Homocedasticidade), dentre outras razões.
"""

